{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "建立Node.js预测模型主要分为以下几个阶段：\n",
    "- 数据的分析及收集\n",
    "- 数据集的清洗及准备\n",
    "- 模型建立\n",
    "- 模型训练\n",
    "- 模型评估\n",
    "\n",
    "其中，模型训练与模型评估为互相循环的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "数据收集\n",
    "我们所要预测的是编写智能合约的Node.js语言，数据的特征有请 老婆 叙述。\n",
    "为了获得足够的有效的智能合约代码，同时规避法律风险，选择在GitHub上抓取相关项目代码。\n",
    "通过对典型智能合约代码的分析，智能合约代码中总是包含两种（得老婆描述）之一。因此，通过搜索代码的方式可以定位到GitHub保存的所有定义智能合约的代码文件。由于本项目旨在预测Node.js语言，语言被锁定在了JavaScript。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate url with base url and params\n",
    "def generate_url(number):\n",
    "    # assign language, page, keyword, type\n",
    "    url_params = \"l=JavaScript&p=\" + str(number) + \"&q=require(fabric-contract-api)+size%3A<3000&type=Code\"\n",
    "    base_url = 'https://github.com/search?'\n",
    "    full_url = base_url + url_params\n",
    "    full_url = urllib.parse.unquote(full_url)\n",
    "    print(full_url)\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "这段代码用于拼接URL，使爬虫自动访问网址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get JavaScript file\n",
    "def get_file_content(count):\n",
    "    driver.get(urls[count])\n",
    "    time.sleep(2)\n",
    "    print(urls[count])\n",
    "    content = driver.page_source\n",
    "    try:\n",
    "        content = content.decode('UTF-8')\n",
    "    except:\n",
    "        print(\"爬取成功\")\n",
    "    else:\n",
    "        print(\"继续...\")\n",
    "    content = etree.HTML(content)\n",
    "    # find file label\n",
    "    file = content.xpath(\"/html/body/div[4]/div/main/div[3]/div/div[3]/div[2]/table/tbody/*\")\n",
    "    \n",
    "    if len(file) <= 0:\n",
    "        file = content.xpath(\"/html/body/div[4]/div/main/div[4]/div/div[3]/div[2]/table/tbody/*\")\n",
    "    \n",
    "    # extract file content by row\n",
    "    file_content = []\n",
    "    for row_num in range(len(file)):\n",
    "        row = ''.join(file[row_num].xpath(\".//td[2]//text()\"))\n",
    "        file_content.append(row + \"\\r\\n\")\n",
    "    # transform into string\n",
    "    file_content = ''.join(file_content)\n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "由于各个项目开放的权限不同导致GitHub界面会有不同的提示，不同访问权限的文件的代码需要定位到不同的div中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract类型的一共有2151个文件，shim类型的有4000个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "由于代码文件中会包含着大量的注释语句，清理数据的第一步便是删除文件中的注释行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bak_file = path + \".bak\";\n",
    "os.rename(path, bak_file);\n",
    "    \n",
    "fp_src = open(bak_file);\n",
    "fp_dst = open(path, 'w');\n",
    "state = S_INIT;\n",
    "for line in fp_src.readlines():\n",
    "    for c in line:\n",
    "        if state == S_INIT:\n",
    "            if c == '/':\n",
    "                state = S_SLASH;\n",
    "            elif c == '\"''\"':\n",
    "                state = S_STR;\n",
    "                fp_dst.write(c);\n",
    "            else:\n",
    "                fp_dst.write(c);\n",
    "        elif state == S_SLASH:\n",
    "            if c == '*':\n",
    "                state = S_BLOCK_COMMENT;\n",
    "            elif c == '/':\n",
    "                state = S_LINE_COMMENT;\n",
    "            else:\n",
    "                fp_dst.write('/');\n",
    "                fp_dst.write(c);\n",
    "                state = S_INIT;\n",
    "        elif state == S_BLOCK_COMMENT:\n",
    "            if c == '*':\n",
    "                state = S_BLOCK_COMMENT_DOT;\n",
    "        elif state == S_BLOCK_COMMENT_DOT:\n",
    "            if c == '/':\n",
    "                state = S_INIT;\n",
    "            else:\n",
    "                state = S_BLOCK_COMMENT;\n",
    "        elif state == S_LINE_COMMENT:\n",
    "            if c == '\\n':\n",
    "                state = S_INIT;\n",
    "        elif state == S_STR:\n",
    "            if c == '\\\\':\n",
    "                state = S_STR_ESCAPE;\n",
    "            elif c == '\"':\n",
    "                state = S_INIT;\n",
    "            fp_dst.write(c);\n",
    "        elif state == S_STR_ESCAPE:\n",
    "            state = S_STR;\n",
    "            fp_dst.write(c);\n",
    "                \n",
    "                \n",
    "fp_src.close();\n",
    "fp_dst.close();\n",
    "os.remove(bak_file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "注释有两种类型，第一种是以“//”开头的一行，第二种是被“/*”和“*/”包裹的多行。本文通过首先将代码复制到一个临时文件中，接着遍历文件中的字符，识别关键的起始符号以确认是否是注释。非注释的代码语句都将被重新写入文件中。最后，临时文件会被删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "本文使用Tokenizer对数据进行分词和编码。Tokenizer使用UTF-8对数据进行读取，在数据的预加载中，定位到了数据中有编码不一致的问题。所以采用下列代码对数据的编码格式进行清洗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bak_file = path + \".bak\";\n",
    "os.rename(path, bak_file);\n",
    "    \n",
    "fp_src = open(bak_file);\n",
    "fp_dst = open(path, 'w', encoding=\"utf-8\");\n",
    "    \n",
    "for line in fp_src.readlines():\n",
    "    if line:\n",
    "        line = line.encode(\"utf-8\", \"ignore\")\n",
    "        fp_dst.write(str(line.decode('utf-8','strict')))\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "fp_src.close()\n",
    "fp_dst.close()\n",
    "os.remove(bak_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "与清理注释类似，本文使用一个指针对每一行进行UTF-8转码并忽略非UTF-8的字符，接着将转码后的行写回文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "数据清洗完毕后，需要对数据中的词和字符进行分词并编码，以便后续模型的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, paths):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "        self.tokenizer.train(trainer, paths)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers包提供了典型的分词方法。本文使用BPE编码。bpe_train()为训练分词模型的方法，它定义了五种特殊的token，用以对特殊字符进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokeniser import BPE_token\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "# the folder 'data' contains all the files\n",
    "paths = [str(x) for x in Path(\"./data/\").glob(\"**/*.js\")]\n",
    "tokenizer = BPE_token()\n",
    "\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(paths)\n",
    "\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "接着使用定义好的分词模型对数据进行分词和编码，并将相关信息保存在文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "# loading tokenizer from the saved model path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "编码完成后，就可以进行模型的训练。首先将先前保存的分词模型加载，并加入五个特殊字符的定义。接着，使用分词模型的词汇表作为配置注入GPT2并形成适应本项目数据的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "for filename in paths:\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        x = f.read()\n",
    "    single_string += x + tokenizer.eos_token\n",
    "\n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "接着将数据集的语句按照编码，形成词汇编码序列，以便模型的输入和输出。本项目将每个文件中的语句连接起来形成一个字符串，其中文件与文件之间加入特殊字符以便标识文件的结尾。接着编码模型便将得到的字符串按照词汇表进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "    examples.append(string_tokenized[i:i + block_size])\n",
    "\n",
    "inputs, labels = [], []\n",
    "\n",
    "for ex in examples:\n",
    "    inputs.append(ex[:-1])\n",
    "    labels.append(ex[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对于已成为编码序列的数据集，数据被分为多个块，本项目将块的大小定义为100，即每一个块有100个词。接着对于每一个块，本项目将数据根据上下文分为输入集和输出集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.8\n",
    "ptr1 = int(len(inputs) * percentage)\n",
    "\n",
    "train_inputs = inputs[:ptr1]\n",
    "test_inputs = inputs[ptr1:]\n",
    "train_labels = labels[:ptr1]\n",
    "test_labels = labels[ptr1:]\n",
    "\n",
    "ptr2 = int(len(train_inputs) * percentage)\n",
    "val_inputs = train_inputs[ptr2:]\n",
    "train_inputs = train_inputs[:ptr2]\n",
    "val_labels = train_labels[ptr2:]\n",
    "train_labels = train_labels[:ptr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "接着是将数据集分割为训练集、验证集和测试集。训练集和测试集按照8:2的比例切分，而训练集中又按照8:2的比例分割成最终的训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels))\n",
    "train_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "数据集准备完毕后，训练集会被TensorFlow根据输入和输出准备好放入模型的数据。而数据会根据设定的buffer size和batch size将数据打乱。本项目中两个值分别为1000和12。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', min_delta=0, patience=10)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=num_epoch, validation_data=val_dataset,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "在模型训练前，优化函数使用了Adam而损失函数使用的是稀疏类别交叉熵。回调函数在模型中被使用以帮助模型在性能无法提升时能够提前终止训练。回调函数监控的指标为验证集的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "最后，训练过程中训练集和验证集的损失被记录并绘制出来，以评估模型的训练过程和训练结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
